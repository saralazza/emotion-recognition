{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2318711,"sourceType":"datasetVersion","datasetId":1399549},{"sourceId":4951650,"sourceType":"datasetVersion","datasetId":2871491},{"sourceId":8684202,"sourceType":"datasetVersion","datasetId":5206420}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install moviepy\n!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:15.652146Z","iopub.execute_input":"2024-06-21T13:29:15.652684Z","iopub.status.idle":"2024-06-21T13:29:39.850035Z","shell.execute_reply.started":"2024-06-21T13:29:15.652640Z","shell.execute_reply":"2024-06-21T13:29:39.848869Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport joblib\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\n\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nfrom mediapipe import solutions\nfrom mediapipe.framework.formats import landmark_pb2\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nfrom moviepy.editor import VideoFileClip\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.851662Z","iopub.execute_input":"2024-06-21T13:29:39.851989Z","iopub.status.idle":"2024-06-21T13:29:39.861836Z","shell.execute_reply.started":"2024-06-21T13:29:39.851958Z","shell.execute_reply":"2024-06-21T13:29:39.860861Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def load_dataset(directory, flag = False):\n    \"\"\"Loads the datasets from the folders \"./datasets/ryerson-emotion-database\" and \"./datasets/ravdess-emotional-speech-video\"\n\n    Return:\n        (list) of paths of the video in the datasets,\n        (list) of different labels found in the dataset \n    \"\"\"\n    paths = []\n\n    for subject in os.listdir(directory):\n        \n        if subject == \".DS_Store\": continue \n        if not flag:\n            percorso_dir_subject = os.path.join(directory, subject)\n        else:\n            percorso_dir_subject = os.path.join(directory, subject, subject)\n\n        for elem in os.listdir(percorso_dir_subject):\n            if elem == \".DS_Store\": continue \n            percorso_elem = os.path.join(percorso_dir_subject, elem)\n\n            if not os.path.isdir(percorso_elem):\n                if not percorso_elem.endswith(\".db\"):\n                    paths.append(percorso_elem)\n            else: \n                for file in os.listdir(percorso_elem):\n                    percorso_file = os.path.join(percorso_elem, file)\n                    if not percorso_file.endswith(\".db\"):\n                        if not flag:\n                            if file.startswith(\"01\") and file[6:8] != \"01\" and file[6:8] != \"02\":\n                                paths.append(percorso_file)\n                        else:\n                            paths.append(percorso_file)\n    \n    return paths","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T13:29:39.863058Z","iopub.execute_input":"2024-06-21T13:29:39.863387Z","iopub.status.idle":"2024-06-21T13:29:39.875564Z","shell.execute_reply.started":"2024-06-21T13:29:39.863358Z","shell.execute_reply":"2024-06-21T13:29:39.874880Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Save video features","metadata":{}},{"cell_type":"code","source":"def save_dataset(features, labels, filepath):\n    \"\"\"Save the processed dataset with all features extracted\n\n    Args:\n        features: list of features for each sample\n        labels: list of label for each sample\n        filepath: target directory of the file\n    \"\"\"\n    joblib.dump((features, labels), filepath)\n    print(f\"Dataset saved in  {filepath}\")\n\ndef load_dataset_jlb(filepath):\n    \"\"\"Load the dataset with all features extracted from the file\n    \n    Args: \n        filepath: file from which load the dataset\n    \n    Return:\n        features: (list) of features for each sample\n        labels: (list) of label for each sample\n\n\n    \"\"\"\n    features, labels = joblib.load(filepath)\n    print(f\"Dataset loaded from {filepath}\")\n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.876836Z","iopub.execute_input":"2024-06-21T13:29:39.877410Z","iopub.status.idle":"2024-06-21T13:29:39.889945Z","shell.execute_reply.started":"2024-06-21T13:29:39.877379Z","shell.execute_reply":"2024-06-21T13:29:39.889004Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# Extract video features","metadata":{}},{"cell_type":"code","source":"def feature_from_Video(detector,frame_video_list):\n    face_blendshapes_scores_list = [] \n    for frame in frame_video_list :\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n        detection_result = detector.detect(image)\n        if detection_result.face_blendshapes == []:\n            if face_blendshapes_scores_list == []:\n                face_blendshapes_scores_list.append(np.zeros(shape= 52))\n            else: face_blendshapes_scores_list.append(face_blendshapes_scores_list[-1])\n        else:\n            face_blendshapes_scores_list.append([cat.score for cat in detection_result.face_blendshapes[0]])\n    return face_blendshapes_scores_list","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.891230Z","iopub.execute_input":"2024-06-21T13:29:39.891510Z","iopub.status.idle":"2024-06-21T13:29:39.903983Z","shell.execute_reply.started":"2024-06-21T13:29:39.891486Z","shell.execute_reply":"2024-06-21T13:29:39.903202Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n    \"\"\"extracts max 50 frames from the video_path\n\n    Args: \n        video_path: path of the video to framerize\n    \n    Returns:\n        returns (list) list of frame shape: 50x240x320x3\n    \"\"\"\n    width = 240\n    height = 320\n    sequence_length = 50\n    frames = []\n\n    video_reader = cv2.VideoCapture(video_path)\n    frame_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    skip_interval = max(int(frame_count/sequence_length), 1)\n\n    for counter in range(sequence_length):\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, counter * skip_interval)\n        ret, frame = video_reader.read\n        if not ret:\n            break\n        frame=cv2.resize(frame, (height, width))\n        frames.append(frame)\n    \n    video_reader.release()\n\n    return frames\n\ndef load_video(paths, dir_path, label_dict):\n    \"\"\"extracts 50 frames from videos in paths\n\n    Args:\n        paths: list of video paths in the datasets\n        labels_name: name of label\n\n    Returns:\n        returns (np.array) shape: len(paths)x50 list of video frame, \n        (np.array) list with associated labels\n    \"\"\"\n    frames_list = []\n    features_list = []\n    labels = []\n    \n    base_options = python.BaseOptions(model_asset_path='/kaggle/input/landmark/face_landmarker_v2_with_blendshapes.task')\n    options = vision.FaceLandmarkerOptions(base_options=base_options,\n                                       output_face_blendshapes=True,\n                                       output_facial_transformation_matrixes=True,\n                                       num_faces=1)\n    detector = vision.FaceLandmarker.create_from_options(options)\n    i = 0\n    for video_path in tqdm(paths, desc = \"loading video\"):\n        \n        frames = frames_extraction(video_path)\n        features = feature_from_Video(detector,frames)\n        features_list.append(features)\n        #frames_list.append(frames)\n        \n        if dir_path[i] == 'RAVDESS':\n            label = video_path.split('/')[-1][6:8]\n            labels.append(int(label)-3)\n        else:\n            label = video_path.split('/')[-1][:2]\n            labels.append(label_dict[label])\n            \n        i+=1\n    \n    return np.array(features_list, dtype='float32'), np.array(labels, dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.910132Z","iopub.execute_input":"2024-06-21T13:29:39.910479Z","iopub.status.idle":"2024-06-21T13:29:39.922755Z","shell.execute_reply.started":"2024-06-21T13:29:39.910456Z","shell.execute_reply":"2024-06-21T13:29:39.921936Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Video model","metadata":{}},{"cell_type":"code","source":"class VideoEmotionRecognitionModel:\n    def __init__(self, input_shape, num_classes, lstm_units=64, dense_units=32, dropout_rate=0.5, learning_rate=0.001):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.lstm_units = lstm_units\n        self.dense_units = dense_units\n        self.dropout_rate = dropout_rate\n        self.learning_rate = learning_rate\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        model.add(LSTM(self.lstm_units, input_shape=self.input_shape, return_sequences=True))\n        model.add(Dropout(self.dropout_rate))\n        model.add(LSTM(self.lstm_units))\n        model.add(Dropout(self.dropout_rate))\n        model.add(Dense(self.dense_units, activation='relu'))\n        model.add(Dense(self.num_classes, activation='softmax'))\n\n        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n                      loss=SparseCategoricalCrossentropy(),\n                      metrics=[SparseCategoricalAccuracy()])\n        return model\n\n    def summary(self):\n        self.model.summary()\n\n    def train(self, X_train, y_train,  X_val, y_val, epochs=50, batch_size=32):\n        checkpoint_callback = ModelCheckpoint(\n            filepath='/kaggle/working/models/video_best_model.keras', \n            monitor='val_sparse_categorical_accuracy',   \n            save_best_only=True,       \n            mode='max',                \n            verbose=1                 \n        )\n\n        history = self.model.fit(\n            X_train, y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=(X_val, y_val),\n            callbacks=[checkpoint_callback]  \n        )\n\n        return history\n    \n    def evaluate(self, X_test, y_test):\n        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n        return test_loss, test_accuracy\n\n    def predict(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predicted_classes\n    \n    def predict_prob(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predictions, predicted_classes\n    \n    def plot_confusion_matrix(self, y_true, y_pred, class_names, title):\n        cm = confusion_matrix(y_true, y_pred)\n        cm = cm / np.sum(cm, axis = 1, keepdims = True)\n        plt.figure(figsize=(10, 7))\n        sns.heatmap(cm, annot=True, fmt='.2%', xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks(rotation=90)\n        plt.yticks(rotation=0)\n        plt.title(title)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.923947Z","iopub.execute_input":"2024-06-21T13:29:39.924304Z","iopub.status.idle":"2024-06-21T13:29:39.945115Z","shell.execute_reply.started":"2024-06-21T13:29:39.924272Z","shell.execute_reply":"2024-06-21T13:29:39.944220Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Train Video Model From Path","metadata":{}},{"cell_type":"code","source":"def train_video_model(train_path, train_dir_name, test_path, test_dir_name, emotions_to_idx):\n    train_dataset_filepath = '/kaggle/working/Video/subtrain_features_rav.pkl'\n    test_dataset_filepath = '/kaggle/working/Video/subtest_features_rav.pkl'\n    os.makedirs('/kaggle/working/Video/', exist_ok = True)\n    \n    if os.path.exists(train_dataset_filepath):\n        train_point_list, train_labels = load_dataset_jlb(train_dataset_filepath)\n        test_point_list, test_labels = load_dataset_jlb(test_dataset_filepath)\n    else:\n        train_point_list, train_labels = load_video(train_path, train_dir_name, emotions_to_idx)\n        save_dataset(train_point_list, train_labels, train_dataset_filepath)\n        \n        test_point_list, test_labels = load_video(test_path, test_dir_name, emotions_to_idx)\n        save_dataset(test_point_list, test_labels, test_dataset_filepath)\n        \n    \n    NUM_CLASSES = len(emotions_to_idx.keys())\n    \n    os.makedirs('/kaggle/working/models/', exist_ok = True)\n\n    emotion_model = VideoEmotionRecognitionModel(input_shape=(50, 52), num_classes=NUM_CLASSES)\n\n    emotion_model.summary()\n\n    history = emotion_model.train(train_point_list, train_labels, test_point_list, test_labels, epochs=70, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.946234Z","iopub.execute_input":"2024-06-21T13:29:39.946526Z","iopub.status.idle":"2024-06-21T13:29:39.959867Z","shell.execute_reply.started":"2024-06-21T13:29:39.946504Z","shell.execute_reply":"2024-06-21T13:29:39.959025Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def video_predict(test_path, test_dir_name, features_file_name, emotions_to_idx, cm_title):\n    \n    os.makedirs('/kaggle/working/Video/', exist_ok = True)\n    labels_name = ['Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n    \n    model_path = '/kaggle/working/models/video_best_model.keras'\n    \n    if os.path.exists(features_file_name):\n        test_point_list, test_labels = load_dataset_jlb(features_file_name)\n    else:\n        test_point_list, test_labels = load_video(test_path, test_dir_name, emotions_to_idx)\n        save_dataset(test_point_list, test_labels, features_file_name)\n        \n    NUM_CLASSES = len(emotions_to_idx.keys())\n    \n    model = VideoEmotionRecognitionModel(input_shape=(50, 52), num_classes=NUM_CLASSES)\n    model.model.load_weights(model_path)\n    \n    model.summary()\n    \n    test_predict_probs, test_predict = model.predict_prob(test_point_list)\n    \n    model.plot_confusion_matrix(test_labels, test_predict, labels_name, cm_title)\n    \n    return test_predict_probs, test_predict\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.960964Z","iopub.execute_input":"2024-06-21T13:29:39.961281Z","iopub.status.idle":"2024-06-21T13:29:39.970435Z","shell.execute_reply.started":"2024-06-21T13:29:39.961250Z","shell.execute_reply":"2024-06-21T13:29:39.969445Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# Audio extraction by Video","metadata":{}},{"cell_type":"code","source":"def extract_audio_from_videos(files, directories, output_directory):\n    out_paths = []\n    rye = output_directory + \"/\" + \"ryerson\"\n    rav = output_directory + \"/\" + \"ravdess\"\n    \n    os.makedirs(output_directory, exist_ok=True)\n    os.makedirs(rye, exist_ok=True)\n    os.makedirs(rav, exist_ok=True)\n    \n    for i, video_path in enumerate(files):\n        filename = video_path.split(\"/\")[-1]\n\n        if directories[i] == \"RAVDESS\":\n            \n            audio_path = os.path.join(rav, filename.split(\".\")[0]+\"_\"+ str(i) +\".wav\")\n        else:\n            audio_path = os.path.join(rye, filename.split(\".\")[0]+ \"_\"+str(i) +\".wav\")\n\n        out_paths.append(audio_path)\n        try:\n            video_clip = VideoFileClip(video_path)\n            audio_clip = video_clip.audio\n            audio_clip.write_audiofile(audio_path, codec='pcm_s16le', logger = None)\n            audio_clip.close()\n            video_clip.close()\n        except Exception as e:\n            print(f\"Failed to process {video_path}: {e}\")\n    return out_paths\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-21T13:29:39.971686Z","iopub.execute_input":"2024-06-21T13:29:39.972252Z","iopub.status.idle":"2024-06-21T13:29:39.985082Z","shell.execute_reply.started":"2024-06-21T13:29:39.972228Z","shell.execute_reply":"2024-06-21T13:29:39.984238Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def load_audio(file_path, sr=22050):\n    dataset = file_path.split(\"/\")[-2]\n    if dataset == \"ravdess\":\n        offset = 1.8\n        duration = 2\n    elif dataset == \"ryerson\":\n        offset = 2\n        duration = 2\n    try:\n        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n        return y, sr\n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file_path)\n        return None, None\n\ndef extract_MFCC(file_path, n_mfcc=64):\n    y, sr = load_audio(file_path)\n    if y is None:\n        return None\n    mfcc_spectrogram = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    if mfcc_spectrogram is not None:\n        if mfcc_spectrogram.shape != (64, 87):\n            if mfcc_spectrogram.shape[1] > 87:\n                mfcc_spectrogram = mfcc_spectrogram[:, :87]\n            else:\n                mfcc_spectrogram = np.pad(mfcc_spectrogram, ((0, 0), (0, 87 - mfcc_spectrogram.shape[1])), mode='constant')\n    return mfcc_spectrogram\n\ndef dump_spectrogram(mel_spectrogram, output_file):\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    np.save(output_file, mel_spectrogram)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:39.986068Z","iopub.execute_input":"2024-06-21T13:29:39.986323Z","iopub.status.idle":"2024-06-21T13:29:39.999620Z","shell.execute_reply.started":"2024-06-21T13:29:39.986301Z","shell.execute_reply":"2024-06-21T13:29:39.998832Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"\ndef preprocess_dataset(files, output_folder):\n    cont = 0\n    ravdess_dict = {3:'ha', 4:'sa', 5:'an', 6:'fe', 7:'di', 8: 'su'}\n    features_path = []\n    for f in files:\n        directory = f.split(\"/\")[-2]\n        filename = f.split(\"/\")[-1].split(\".\")[0]\n        if directory == \"ryerson\":\n                emotion = filename[:2]\n                audio_file = f\n                output_file = os.path.join(\n                    output_folder, emotion,\n                    filename + \".npy\",\n                )\n                mfcc_spectrogram = extract_MFCC(audio_file)\n                dump_spectrogram(mfcc_spectrogram, output_file)\n                cont+=1\n\n        elif directory == \"ravdess\": # ravdess\n                part = filename.split('-')\n                emotion = ravdess_dict[int(part[2])]\n                audio_file = f\n                output_file = os.path.join(\n                    output_folder, emotion,\n                    filename + \".npy\",\n                )\n                mfcc_spectrogram = extract_MFCC(audio_file)\n                dump_spectrogram(mfcc_spectrogram, output_file)\n                cont+=1\n                \n        features_path.append(output_file) \n    return features_path\n    print(cont, \" Audio caricati\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.000857Z","iopub.execute_input":"2024-06-21T13:29:40.001123Z","iopub.status.idle":"2024-06-21T13:29:40.015120Z","shell.execute_reply.started":"2024-06-21T13:29:40.001101Z","shell.execute_reply":"2024-06-21T13:29:40.014168Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"\nclass MultilabelSpectrogramDataset(Dataset):\n    def __init__(self, file_paths, labels):\n        self.file_paths = file_paths\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        spectrogram = np.load(file_path)\n        label = self.labels[idx]\n        return torch.tensor(spectrogram, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.016193Z","iopub.execute_input":"2024-06-21T13:29:40.016509Z","iopub.status.idle":"2024-06-21T13:29:40.027920Z","shell.execute_reply.started":"2024-06-21T13:29:40.016481Z","shell.execute_reply":"2024-06-21T13:29:40.027063Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def load_audio_dataset(files):\n    labels = []\n    for file in files:\n        label_name = file.split(\"/\")[-2]\n        label = emotions_to_idx[label_name]\n        labels.append(label)\n    return labels","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.028993Z","iopub.execute_input":"2024-06-21T13:29:40.029230Z","iopub.status.idle":"2024-06-21T13:29:40.038556Z","shell.execute_reply.started":"2024-06-21T13:29:40.029209Z","shell.execute_reply":"2024-06-21T13:29:40.037622Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"class AudioModel(nn.Module):\n    def __init__(self, num_labels=6):\n        super(AudioModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=\"same\")\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=\"same\")\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=\"same\")\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=\"same\")\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n\n        self.fc1 = nn.Linear(128 * 4 * 5, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.fc4 = nn.Linear(128, num_labels)\n\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv3(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv4(x))\n        x = self.pool(x)\n\n\n        x = x.flatten(start_dim=1)\n\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc4(x)\n        x = self.softmax(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.039618Z","iopub.execute_input":"2024-06-21T13:29:40.039927Z","iopub.status.idle":"2024-06-21T13:29:40.052853Z","shell.execute_reply.started":"2024-06-21T13:29:40.039904Z","shell.execute_reply":"2024-06-21T13:29:40.051864Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"code","source":"def train_audio_model(subtrain_dataloader, subtest_dataloader):\n    \n    model = AudioModel()\n    \n    # Labels names for the plots\n    labels_list = ['Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n    \n    num_labels = len(labels_list)\n    confusion_matrix_ = np.zeros((num_labels, num_labels))\n\n    os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=0.0001, weight_decay=0.0001)\n\n    losses = []\n    accuracy_scores = dict()\n\n    num_epochs = 70\n\n    for epoch in range(num_epochs):\n        model.train()\n\n        total_loss = 0.0\n\n        for audios, labels in tqdm(subtrain_dataloader):\n            audios = audios.to(device)\n            labels = labels.to(device)\n\n            outputs = model(audios)\n            loss = criterion(outputs, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}')\n\n        model.eval()\n\n        all_preds = []\n        all_labels = []\n\n        for audios, labels in subtest_dataloader:\n            audios = audios.to(device)\n            labels = labels.to(device)\n\n            outputs = model(audios)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        print(f'Validation accuracy: {accuracy}')\n\n        accuracy_scores[accuracy] = model.state_dict()\n\n        losses.append(total_loss)\n\n    best_accuracy = max(accuracy_scores.keys())\n    best_model = accuracy_scores[best_accuracy]\n    print(\"\\nBest validation accuracy: \", best_accuracy, \"\\n\")\n    torch.save(best_model, '/kaggle/working/models/audio_model.pth')\n    accuracy_scores = list(accuracy_scores.keys())\n\n    # Plot Training Loss\n    plt.plot(losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Audio training Loss')\n    plt.show()\n\n    # Plot Validation Accuracy\n    plt.plot(accuracy_scores)\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Audio validation Accuracy')\n    plt.show()\n\n    confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n    \n    # confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap='Purples')\n    plt.title('Audio prediction on subtest set on training mode')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.053939Z","iopub.execute_input":"2024-06-21T13:29:40.054345Z","iopub.status.idle":"2024-06-21T13:29:40.070206Z","shell.execute_reply.started":"2024-06-21T13:29:40.054322Z","shell.execute_reply":"2024-06-21T13:29:40.069338Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def create_audio_model():\n    \n    print(\"Audio extraction by video of subtrain set...\\n\")\n    audio_paths_subtrain = extract_audio_from_videos(subtrain_data, subtrain_dir, '/kaggle/working/Audios/subtrain')\n    print(\"Audio extraction by video of subtest set...\\n\")\n    audio_paths_subtest = extract_audio_from_videos(subtest_data, subtest_dir, '/kaggle/working/Audios/subtest')\n    \n    print(\"Features extraction by audio of subtrain set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/subtrain\"\n    features_paths_subtrain = preprocess_dataset(audio_paths_subtrain, output_folder)\n\n    print(\"Features extraction by audio of subtest set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/subtest\"\n    features_paths_subtest = preprocess_dataset(audio_paths_subtest, output_folder)\n    \n    \n    # estrazione delle label delle features\n    subtrain_labels = load_audio_dataset(features_paths_subtrain)\n    subtest_labels = load_audio_dataset(features_paths_subtest)\n    \n    \n    BATCH_SIZE = 32\n\n    subtrain_dataset = MultilabelSpectrogramDataset(features_paths_subtrain, subtrain_labels)\n\n    subtest_dataset = MultilabelSpectrogramDataset(features_paths_subtest, subtest_labels)\n    \n    subtrain_dataloader = DataLoader(subtrain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    subtest_dataloader = DataLoader(subtest_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    \n    print(\"Training audio model...\\n\")\n    train_audio_model(subtrain_dataloader, subtest_dataloader)\n    \n    return subtest_dataloader, subtest_labels","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.071167Z","iopub.execute_input":"2024-06-21T13:29:40.071425Z","iopub.status.idle":"2024-06-21T13:29:40.084986Z","shell.execute_reply.started":"2024-06-21T13:29:40.071404Z","shell.execute_reply":"2024-06-21T13:29:40.084058Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def audio_predict(subtest_dataloader_audio):\n    # audio, fetature and label extraction by videos \n\n    print(\"Audio extraction by video of test set...\\n\")\n    audio_paths_test = extract_audio_from_videos(test_data, test_dir, '/kaggle/working/Audios/test')\n    \n    print(\"Features extraction by audio of test set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/test\"\n    features_paths_test = preprocess_dataset(audio_paths_test, output_folder)\n    \n    # label extraction by test features\n    test_labels = load_audio_dataset(features_paths_test)\n    \n    \n    BATCH_SIZE = 32\n\n    #  dataset creation with [spectogram, label]\n    test_dataset = MultilabelSpectrogramDataset(features_paths_test, test_labels)\n    \n    \n    # converting dataset in dataloader\n    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    print(\"Loading of audio model by file...\\n\")\n    model_path = '/kaggle/working/models/audio_model.pth'\n\n    model = AudioModel()\n\n    model.load_state_dict(torch.load(model_path))\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    model.eval()\n    \n    \n    labels_list = ['Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n    num_labels = len(labels_list)\n    confusion_matrix_ = np.zeros((num_labels, num_labels))\n    subtest_preds = []\n    all_labels = []\n    all_outputs_subtest = []\n\n    for audios, labels in subtest_dataloader_audio:\n        audios = audios.to(device)\n        labels = labels.to(device)\n\n        outputs = model(audios)\n        all_outputs_subtest.extend(outputs.cpu().detach().numpy())\n        _, preds = torch.max(outputs, 1)\n\n        subtest_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n    accuracy = accuracy_score(all_labels, subtest_preds)\n    print(f'\\n\\nAccuracy of audio model on subtest set: {accuracy}\\n')\n    \n    confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap = \"Purples\")\n    plt.title('Audio prediction on subtest set')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    \n    confusion_matrix_ = np.zeros((num_labels, num_labels))\n    test_preds = []\n    all_labels = []\n    all_outputs_test = []\n    for audios, labels in test_dataloader:\n        audios = audios.to(device)\n        labels = labels.to(device)\n\n        outputs = model(audios)\n        _, preds = torch.max(outputs, 1)\n        \n        all_outputs_test.extend(outputs.cpu().detach().numpy())\n        test_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n    accuracy = accuracy_score(all_labels, test_preds)\n    print(f'\\n\\nAccuracy of audio model on test set: {accuracy}\\n')\n    \n    confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap='Purples')\n    plt.title('Audio prediction on test set')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    return subtest_preds, test_preds, test_labels, all_outputs_subtest, all_outputs_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.086277Z","iopub.execute_input":"2024-06-21T13:29:40.086587Z","iopub.status.idle":"2024-06-21T13:29:40.104963Z","shell.execute_reply.started":"2024-06-21T13:29:40.086564Z","shell.execute_reply":"2024-06-21T13:29:40.103984Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# Terzo Modello","metadata":{}},{"cell_type":"code","source":"class VideoAudioDataset(Dataset):\n    def __init__(self, preds, labels):\n        self.preds = preds\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        pred = self.preds[idx]\n        label = self.labels[idx]\n        return torch.tensor(pred, dtype=torch.float32), torch.tensor(label, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.105998Z","iopub.execute_input":"2024-06-21T13:29:40.106280Z","iopub.status.idle":"2024-06-21T13:29:40.118464Z","shell.execute_reply.started":"2024-06-21T13:29:40.106258Z","shell.execute_reply":"2024-06-21T13:29:40.117616Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"class VideoAudioModel(nn.Module):\n    def __init__(self, num_labels=6):\n        super(VideoAudioModel, self).__init__()\n        self.fc1 = nn.Linear(12,128)\n        self.fc2 = nn.Linear(128,256)\n        self.fc3 = nn.Linear(256,512)\n        self.fc4 = nn.Linear(512,128)\n        self.fc5 = nn.Linear(128,64)\n        self.fc6 = nn.Linear(64,num_labels)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.dropout(torch.relu(self.fc2(x)))\n        x = self.dropout(torch.relu(self.fc3(x)))\n        x = self.dropout(torch.relu(self.fc4(x)))\n        x = self.dropout(torch.relu(self.fc5(x)))\n        x = self.fc6(x)\n        return x\n    \n    def train_model(self, train_dataloader, test_dataloader):\n        labels_list = ['Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n        num_labels = len(labels_list)\n        confusion_matrix_ = np.zeros((num_labels, num_labels))\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.to(device)\n        losses = []\n        accuracy_scores = dict()\n        \n        num_epochs = 100\n        \n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(\n            self.parameters(), lr=0.0001, weight_decay=0.0001)\n        \n        last_model = self.state_dict()\n        \n        for epoch in range(num_epochs):\n            self.train()\n            total_loss = 0.0\n            for inputs, labels in tqdm(train_dataloader):\n                pred = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = self(pred)\n                loss = criterion(outputs, labels)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}')\n\n            self.eval()\n\n            all_preds = []\n            all_labels = []\n\n            for inputs, labels in test_dataloader:\n                pred = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = self(pred)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n            accuracy = accuracy_score(all_labels, all_preds)\n            print(f'Validation accuracy: {accuracy}')\n\n            accuracy_scores[accuracy] = self.state_dict()\n\n            losses.append(total_loss)\n            \n            best_accuracy = max(accuracy_scores.keys())\n            \n            last_model = self.state_dict()\n            \n        best_model = accuracy_scores[best_accuracy]\n        print(\"\\n\\nBest validation Audio-Video model accuracy: \", best_accuracy, \"\\n\")\n        torch.save(best_model, '/kaggle/working/models/audio_video_model.pth')\n        accuracy_scores = list(accuracy_scores.keys())\n        \n        torch.save(last_model, '/kaggle/working/models/last_epoch_audio_video_model.pth')\n\n        plt.plot(losses)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Audio-video training Loss')\n        plt.show()\n        \n        plt.plot(accuracy_scores)\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Audio-video validation Accuracy')\n        plt.show()\n\n        confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap = \"Blues\")\n        plt.title('Video-Audio prediction on test set in training mode')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n        return\n    \n    def test_model(self, test_dataloader):\n        labels_list = ['Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n        num_labels = len(labels_list)\n        confusion_matrix_ = np.zeros((num_labels, num_labels))\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.to(device)\n        model_path = '/kaggle/working/models/audio_video_model.pth'\n        self.load_state_dict(torch.load(model_path))\n        self.eval()\n\n        all_preds = []\n        all_labels = []\n        all_outputs = []\n\n        for inputs, labels in test_dataloader:\n            pred = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = self(pred)\n            all_outputs.append(outputs)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        print(f'\\n\\nAudio-Video model accuracy on test set: {accuracy}')\n\n        confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap=\"Blues\")\n        plt.title('Video-Audio prediction on test set')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n        return all_preds, all_outputs\n            ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.120001Z","iopub.execute_input":"2024-06-21T13:29:40.120327Z","iopub.status.idle":"2024-06-21T13:29:40.149413Z","shell.execute_reply.started":"2024-06-21T13:29:40.120299Z","shell.execute_reply":"2024-06-21T13:29:40.148569Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"path_RAVDESS = load_dataset('/kaggle/input/ravdess-emotional-speech-video/RAVDESS dataset')\npath_RYERSON = load_dataset('/kaggle/input/ryerson-emotion-database', True)\npath = path_RAVDESS + path_RYERSON\n\ndir_RAVDESS = [\"RAVDESS\"]*len(path_RAVDESS)\ndir_RYERSON = [\"RYERSON\"]*len(path_RYERSON)\ndir_name = dir_RAVDESS + dir_RYERSON\n\ntrain_data, test_data, train_dir, test_dir = train_test_split(path, dir_name, test_size=0.1, random_state= 15)\n\nprint(\"Submodels train + Audio-video model train size: \", len(train_data))\nprint(\"Audio-video model test size: \", len(test_data))\nprint()\n\nsubtrain_data, subtest_data, subtrain_dir, subtest_dir = train_test_split(train_data, train_dir, test_size=0.3, random_state= 15)\n\nprint(\"Submodels train size: \", len(subtrain_data))\nprint(\"Submodels test size: \", len(subtest_data))\n\nemotions_to_idx = {'ha': 0, 'sa': 1, 'an': 2, 'fe': 3, 'di': 4, 'su': 5}\n\nvideo_model_path = '/kaggle/working/models/video_best_model.keras'\naudio_model_path = '/kaggle/working/models/audio_model.pth'\n\ntrain_video_model(subtrain_data, subtrain_dir, subtest_data, subtest_dir, emotions_to_idx)\nsubtest_dataloader_audio, subtest_labels = create_audio_model()\n\nsubtest_video_predict_probs, subtest_video_predict  = video_predict(subtest_data, subtest_dir, '/kaggle/working/Video/subtest_features_rav.pkl', emotions_to_idx, \"Video prediction on subtest set\")\ntest_video_predict_probs, test_video_predict = video_predict(test_data, test_dir, '/kaggle/working/Video/test_features_rav.pkl', emotions_to_idx, \"Video prediction on test set\")\n\nsubtest_audio_predict, test_audio_predict , test_labels, subtest_audio_predict_probs, test_audio_predict_probs = audio_predict(subtest_dataloader_audio)\n\nsubtest_video_predict_numpy = np.array(subtest_video_predict_probs)\nsubtest_audio_predict_numpy = np.array(subtest_audio_predict_probs)\nsubtest_audioVideo_predict = np.column_stack((subtest_audio_predict_numpy, subtest_video_predict_numpy))\n\ntest_video_predict_numpy = np.array(test_video_predict_probs)\ntest_audio_predict_numpy = np.array(test_audio_predict_probs)\ntest_audioVideo_predict = np.column_stack((test_audio_predict_numpy, test_video_predict_numpy))\n\nsubtest_dataset = VideoAudioDataset(subtest_audioVideo_predict, subtest_labels)\n\ntest_dataset = VideoAudioDataset(test_audioVideo_predict, test_labels)\n\nBATCH_SIZE = 32\n\nsubtest_dataloader = DataLoader(subtest_dataset, batch_size=BATCH_SIZE, shuffle = False)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = False)\n\naudioVideo_model = VideoAudioModel()\naudioVideo_model.train_model(subtest_dataloader, test_dataloader)\n\ndef_preds, def_outputs = audioVideo_model.test_model(test_dataloader)\n\ntrue_labels = np.array(test_labels)\npredictions_model_1 = np.array(test_audio_predict)\npredictions_model_2 = np.array(test_video_predict)\npredictions_model_3 = np.array(def_preds)\n\ncorrectness = np.zeros((len(test_labels), 3)) \n\ncorrectness[:, 0] = (predictions_model_1 == true_labels)\ncorrectness[:, 1] = (predictions_model_3 == true_labels)\ncorrectness[:, 2] = (predictions_model_2 == true_labels)\n\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(correctness, annot=False, cmap='Greens', cbar=False, xticklabels=['Audio', 'Audio-Video','Video'], yticklabels=[])\nplt.xlabel('Model')\nplt.ylabel('Predictions')\nplt.title('Correctness of Predictions by Models')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:29:40.151646Z","iopub.execute_input":"2024-06-21T13:29:40.152002Z","iopub.status.idle":"2024-06-21T13:35:36.770164Z","shell.execute_reply.started":"2024-06-21T13:29:40.151960Z","shell.execute_reply":"2024-06-21T13:35:36.769289Z"}},"execution_count":null,"outputs":[]}]}