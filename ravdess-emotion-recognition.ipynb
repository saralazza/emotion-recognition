{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4951650,"sourceType":"datasetVersion","datasetId":2871491},{"sourceId":8684202,"sourceType":"datasetVersion","datasetId":5206420},{"sourceId":10550454,"sourceType":"datasetVersion","datasetId":6527904},{"sourceId":263161,"sourceType":"modelInstanceVersion","modelInstanceId":225064,"modelId":246788},{"sourceId":263162,"sourceType":"modelInstanceVersion","modelInstanceId":225065,"modelId":246789},{"sourceId":263164,"sourceType":"modelInstanceVersion","modelInstanceId":225067,"modelId":246792},{"sourceId":263165,"sourceType":"modelInstanceVersion","modelInstanceId":225068,"modelId":246793}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pydub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:26.282704Z","iopub.execute_input":"2025-03-05T18:40:26.283107Z","iopub.status.idle":"2025-03-05T18:40:33.244185Z","shell.execute_reply.started":"2025-03-05T18:40:26.283057Z","shell.execute_reply":"2025-03-05T18:40:33.242841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport joblib\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\nimport pandas as pd\n\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nfrom mediapipe import solutions\nfrom mediapipe.framework.formats import landmark_pb2\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, GRU, LSTM, Dropout, Dense, Conv1D, MaxPooling1D, BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import Model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom pydub.utils import which\n\nfrom pydub import AudioSegment\n\nos.system('apt-get install -y ffmpeg')\n\nAudioSegment.converter = which(\"ffmpeg\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:33.245816Z","iopub.execute_input":"2025-03-05T18:40:33.246146Z","iopub.status.idle":"2025-03-05T18:40:35.355511Z","shell.execute_reply.started":"2025-03-05T18:40:33.246119Z","shell.execute_reply":"2025-03-05T18:40:35.354772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"import os\n\ndef load_dataset(directory):\n    \"\"\"Loads the datasets from the folders \"./datasets/ravdess-emotional-speech-video\".\n\n    Args:\n        directory: The name of the dataset directory.\n\n    Returns:\n        A tuple containing:\n        - A list of paths to the videos in the dataset.\n        - A list of relative labels corresponding to each video.\n    \"\"\"\n    paths = []  # List to store the paths of the videos\n    labels = []  # List to store the labels corresponding to each video\n\n    # Iterate over each subject directory in the dataset\n    for subject in os.listdir(directory):\n        # Skip the \".DS_Store\" file if present\n        if subject == \".DS_Store\":\n            continue\n\n        # Construct the path to the subject directory\n        percorso_dir_subject = os.path.join(directory, subject)\n\n        # Iterate over each element (e.g., video files) in the subject directory\n        for elem in os.listdir(percorso_dir_subject):\n            # Skip the \".DS_Store\" file if present\n            if elem == \".DS_Store\":\n                continue\n\n            # Construct the path to the element\n            percorso_elem = os.path.join(percorso_dir_subject, elem)\n\n            # Iterate over each file in the element directory\n            for file in os.listdir(percorso_elem):\n                # Construct the full path to the file\n                percorso_file = os.path.join(percorso_elem, file)\n\n                # Check if the file is not a \".db\" file and starts with \"01\"\n                if not percorso_file.endswith(\".db\") and file.startswith(\"01\"):\n                    # Add the file path to the list of paths\n                    paths.append(percorso_file)\n\n                    # Extract the label from the file name and add it to the list of labels\n                    label = percorso_file.split('/')[-1][6:8]\n                    labels.append(int(label) - 1)\n\n    return paths, labels  # Return the list of paths and labels\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:37:25.253829Z","iopub.execute_input":"2025-03-05T19:37:25.254284Z","iopub.status.idle":"2025-03-05T19:37:25.261845Z","shell.execute_reply.started":"2025-03-05T19:37:25.254249Z","shell.execute_reply":"2025-03-05T19:37:25.260703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save video features","metadata":{}},{"cell_type":"code","source":"def save_dataset(features, labels, filepath):\n    \"\"\"Save the processed dataset with all features extracted\n\n    Args:\n        features: list of features for each sample\n        labels: list of label for each sample\n        filepath: target directory of the file\n    \"\"\"\n    joblib.dump((features, labels), filepath)\n    print(f\"Dataset saved in  {filepath}\")\n\ndef load_dataset_jlb(filepath):\n    \"\"\"Load the dataset with all features extracted from the file\n    \n    Args: \n        filepath: file from which load the dataset\n    \n    Return:\n        features: (list) of features for each sample\n        labels: (list) of label for each sample\n    \"\"\"\n    features, labels = joblib.load(filepath)\n    print(f\"Dataset loaded from {filepath}\")\n    return features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.364759Z","iopub.execute_input":"2025-03-05T18:40:35.365108Z","iopub.status.idle":"2025-03-05T18:40:35.380649Z","shell.execute_reply.started":"2025-03-05T18:40:35.365071Z","shell.execute_reply":"2025-03-05T18:40:35.379746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Utils","metadata":{}},{"cell_type":"code","source":"def load_data(paths):\n    \"\"\"Loads features files and obtain the relative labels for each video path\n\n    Args:\n        paths: list of .csv files\n\n    Return:\n        features: array of features of files\n        labels: array of labels of files\n    \"\"\"\n    \n    features = []\n    labels = []\n\n    sequence_length = 50\n\n    for video_path in tqdm(paths, desc = \"loading video features\"):\n        data = pd.read_csv(video_path)\n        data = data.drop(columns=['timestamp', 'frame', 'confidence'])\n        h = data.shape[0]\n        skip_interval = max(int(h/sequence_length), 1)\n        \n        data = data[:skip_interval*sequence_length:skip_interval]\n        features.append(data.values)\n        \n        label = video_path.split('/')[-1][6:8]\n        labels.append(int(label)-1)\n\n    features = np.array(features, dtype='float32')\n    labels = np.array(labels, dtype='int8')\n    \n    return features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.381454Z","iopub.execute_input":"2025-03-05T18:40:35.381778Z","iopub.status.idle":"2025-03-05T18:40:35.389456Z","shell.execute_reply.started":"2025-03-05T18:40:35.381741Z","shell.execute_reply":"2025-03-05T18:40:35.388719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def from_audiopath_to_videopath(paths_audio):\n    \"\"\" Obtain features paths from video/audio paths\n\n    Args:\n        paths_audio: list of video/audio paths\n\n    Return:\n        paths_video: list of features video paths\n    \"\"\"\n\n    \"\"\"\n        From original video path: /kaggle/input/ravdess-emotional-speech-video/RAVDESS dataset/Video_Speech_Actor_01/Actor_01/01-01-01-01-01-01-01.mp4\n        To features video path: /kaggle/input/ravdess-facial-tracking/01-01-01-01-01-01-01.csv\n    \"\"\"\n    \n    dir_video = '/kaggle/input/ravdess-facial-tracking/'\n    paths_video = []\n    for path in paths_audio:\n        name = path.split('/')[-1].split('.')[0]\n        path_video = dir_video + name + '.csv'\n        paths_video.append(path_video)\n\n    return paths_video","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.390324Z","iopub.execute_input":"2025-03-05T18:40:35.390585Z","iopub.status.idle":"2025-03-05T18:40:35.406272Z","shell.execute_reply.started":"2025-03-05T18:40:35.390565Z","shell.execute_reply":"2025-03-05T18:40:35.405378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Model","metadata":{}},{"cell_type":"markdown","source":"### GRU Model","metadata":{}},{"cell_type":"code","source":"class GRUEmotionRecognitionModel:\n    def __init__(self, input_shape, num_classes, gru_units=64, dense_units=64, dropout_rate=0.5, learning_rate=0.001):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.gru_units = gru_units\n        self.dense_units = dense_units\n        self.dropout_rate = dropout_rate\n        self.learning_rate = learning_rate\n        self.model = self._build_model()\n    \n    def _build_model(self):\n        model = Sequential()\n        \n        # Convolutional Layer\n        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=self.input_shape))\n        model.add(MaxPooling1D(pool_size=2))\n        \n        # GRU Layer\n        model.add(GRU(self.gru_units, return_sequences=True))\n        model.add(Dropout(self.dropout_rate))\n        model.add(BatchNormalization())\n\n        # Dense Layers\n        model.add(Flatten())\n        model.add(Dense(self.dense_units * 2, activation='relu'))\n        model.add(Dropout(self.dropout_rate))\n        model.add(BatchNormalization())\n        model.add(Dense(self.num_classes, activation='softmax'))\n        \n        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n                      loss=SparseCategoricalCrossentropy(),\n                      metrics=[SparseCategoricalAccuracy()])\n        return model\n\n    def summary(self):\n        self.model.summary()\n    \n    def train(self, X_train, y_train,  X_val, y_val, epochs=100, batch_size=64):\n        checkpoint_callback = ModelCheckpoint(\n            filepath='/kaggle/working/models/video_best_model_GRU.keras', \n            monitor='val_sparse_categorical_accuracy',   \n            save_best_only=True,       \n            mode='max',                \n            verbose=1                 \n        )\n        \n        history = self.model.fit(\n            X_train, y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=(X_val, y_val),\n            callbacks=[checkpoint_callback]  \n        )\n        \n        return history\n    \n    def evaluate(self, X_test, y_test):\n        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n        return test_loss, test_accuracy\n    \n    def predict(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predicted_classes\n    \n    def predict_prob(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predictions, predicted_classes\n    \n    def plot_confusion_matrix(self, y_true, y_pred, class_names, title):\n        cm = confusion_matrix(y_true, y_pred)\n        cm = cm / np.sum(cm, axis = 1, keepdims = True)\n        plt.figure(figsize=(10, 7))\n        sns.heatmap(cm, annot=True, fmt='.2%', xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks(rotation=90)\n        plt.yticks(rotation=0)\n        plt.title(title)\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.407124Z","iopub.execute_input":"2025-03-05T18:40:35.407370Z","iopub.status.idle":"2025-03-05T18:40:35.419269Z","shell.execute_reply.started":"2025-03-05T18:40:35.407349Z","shell.execute_reply":"2025-03-05T18:40:35.418298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LSTM Model","metadata":{}},{"cell_type":"code","source":"class LSTMEmotionRecognitionModel:\n    def __init__(self, input_shape, num_classes, lstm_units=64, dense_units=64, dropout_rate=0.5, learning_rate=0.001):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.lstm_units = lstm_units\n        self.dense_units = dense_units\n        self.dropout_rate = dropout_rate\n        self.learning_rate = learning_rate\n        self.model = self._build_model()\n    \n    def _build_model(self):\n        model = Sequential()\n        \n        # Convolutional Layer\n        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=self.input_shape))\n        model.add(MaxPooling1D(pool_size=2))\n        \n        # First LSTM Layer\n        model.add(LSTM(self.lstm_units, return_sequences=True))\n        model.add(Dropout(self.dropout_rate))\n        model.add(BatchNormalization())\n    \n        # Dense Layers\n        model.add(Flatten())\n        model.add(Dense(self.dense_units * 2, activation='relu'))\n        model.add(Dropout(self.dropout_rate))\n        model.add(BatchNormalization())\n        model.add(Dense(self.num_classes, activation='softmax'))\n        \n        model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n                      loss=SparseCategoricalCrossentropy(),\n                      metrics=[SparseCategoricalAccuracy()])\n        return model\n\n    def summary(self):\n        self.model.summary()\n    \n    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64):\n        checkpoint_callback = ModelCheckpoint(\n            filepath='/kaggle/working/models/video_best_model_LSTM.keras', \n            monitor='val_sparse_categorical_accuracy',   \n            save_best_only=True,       \n            mode='max',                \n            verbose=1                 \n        )\n        \n        history = self.model.fit(\n            X_train, y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=(X_val, y_val),\n            callbacks=[checkpoint_callback]  \n        )\n        \n        return history\n    \n    def evaluate(self, X_test, y_test):\n        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n        return test_loss, test_accuracy\n    \n    def predict(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predicted_classes\n    \n    def predict_prob(self, X):\n        predictions = self.model.predict(X)\n        predicted_classes = np.argmax(predictions, axis=1)\n        return predictions, predicted_classes\n    \n    def plot_confusion_matrix(self, y_true, y_pred, class_names, title):\n        cm = confusion_matrix(y_true, y_pred)\n        cm = cm / np.sum(cm, axis=1, keepdims=True)\n        plt.figure(figsize=(10, 7))\n        sns.heatmap(cm, annot=True, fmt='.2%', xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks(rotation=90)\n        plt.yticks(rotation=0)\n        plt.title(title)\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.422004Z","iopub.execute_input":"2025-03-05T18:40:35.422322Z","iopub.status.idle":"2025-03-05T18:40:35.434907Z","shell.execute_reply.started":"2025-03-05T18:40:35.422299Z","shell.execute_reply":"2025-03-05T18:40:35.434001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Test Video Model","metadata":{}},{"cell_type":"code","source":"def train_video_model(train_path, test_path, emotions_to_idx):\n    \"\"\"Load and normalize the train and test dataset and train GRU and LSTM video model.\n\n    Args:\n        train_path: paths of the .csv files used to train the video model\n        test_path: paths of the .csv files used to test the video model\n        emotions_to_idx: dictonary to convert the name of the emotion to the relative index\n\n    Return:\n        scaler: the Scaler object to normalize train and test data\n    \"\"\"\n    \n    train_dataset_filepath = '/kaggle/working/Video_track/subtrain_features_rav.pkl'\n    test_dataset_filepath = '/kaggle/working/Video_track/subtest_features_rav.pkl'\n    os.makedirs('/kaggle/working/Video_track/', exist_ok = True)\n\n    # Load train and test dataset\n    if os.path.exists(train_dataset_filepath):\n        train_point_list, train_labels = load_dataset_jlb(train_dataset_filepath)\n        test_point_list, test_labels = load_dataset_jlb(test_dataset_filepath)\n\n    else:\n        train_point_list, train_labels = load_data(train_path)\n        save_dataset(train_point_list, train_labels, train_dataset_filepath)\n        test_point_list, test_labels = load_data(test_path)\n        save_dataset(test_point_list, test_labels, test_dataset_filepath)\n\n    # Normalize dataset\n    scaler = MinMaxScaler()\n    train_point_list = scaler.fit_transform(train_point_list.reshape(-1, train_point_list.shape[-1])).reshape(train_point_list.shape)\n    test_point_list = scaler.transform(test_point_list.reshape(-1, test_point_list.shape[-1])).reshape(test_point_list.shape)\n    \n    NUM_CLASSES = len(emotions_to_idx.keys())\n\n    os.makedirs('/kaggle/working/models/', exist_ok = True)\n\n    # If the model file doesn't exist, train GRU video model and save on the file\n    if not os.path.exists('/kaggle/working/models/video_best_model_GRU.keras'):\n        gru_emotion_model = GRUEmotionRecognitionModel(input_shape=(50, 709), num_classes=NUM_CLASSES)\n        gru_emotion_model.summary()\n        history = gru_emotion_model.train(train_point_list, train_labels, test_point_list, test_labels, epochs=100, batch_size=64)\n\n    # If the model file doesn't exist, train LSTM video model and save on the file\n    if not os.path.exists('/kaggle/working/models/video_best_model_LSTM.keras'):\n        lstm_emotion_model = LSTMEmotionRecognitionModel(input_shape=(50, 709), num_classes=NUM_CLASSES)\n        lstm_emotion_model.summary()\n        history = lstm_emotion_model.train(train_point_list, train_labels, test_point_list, test_labels, epochs=100, batch_size=64)\n\n    return scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.436720Z","iopub.execute_input":"2025-03-05T18:40:35.436945Z","iopub.status.idle":"2025-03-05T18:40:35.454744Z","shell.execute_reply.started":"2025-03-05T18:40:35.436926Z","shell.execute_reply":"2025-03-05T18:40:35.454110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def video_predict(scaler, test_path, features_file_name, emotions_to_idx, cm_title):\n    \"\"\"Loads the datasets from the folders \"./datasets/ravdess-emotional-speech-video\"\n\n    Args:\n        scaler: the Scaler object to normalize train and test data\n        test_path: paths of the .csv files used to test the video model\n        features_file_name: path of the file of features\n        emotions_to_idx: dictonary to convert the name of the emotion to the relative index\n        cm_title: the title for the confusion matrix\n\n    Return:\n        gru_test_predict_probs: the logits predicted by the GRU model\n        gru_test_predict: the classes predicted by the GRU model\n        lstm_test_predict_probs: the logits predicted by the LSTM model\n        lstm_test_predict: the classes predicted by the LSTM model\n    \"\"\"\n    \n    os.makedirs('/kaggle/working/Video_track/', exist_ok = True)\n    labels_name = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n    \n    lstm_path = '/kaggle/working/models/video_best_model_LSTM.keras'\n    gru_path = '/kaggle/working/models/video_best_model_GRU.keras'\n\n    # Load test dataset\n    if os.path.exists(features_file_name):\n        test_point_list, test_labels = load_dataset_jlb(features_file_name)\n    else:\n        test_point_list, test_labels = load_data(test_path)\n        save_dataset(test_point_list, test_labels, features_file_name)\n        \n    NUM_CLASSES = len(emotions_to_idx.keys())\n\n    # Normalize test data\n    test_point_list = scaler.transform(test_point_list.reshape(-1, test_point_list.shape[-1])).reshape(test_point_list.shape)\n\n    # Load parameters of GRU video model, make the predictions and plot the confusion matrix\n    gru_model = GRUEmotionRecognitionModel(input_shape=(50, 709), num_classes=NUM_CLASSES)\n    gru_model.model.load_weights(gru_path)\n    gru_model.summary()\n    gru_test_predict_probs, gru_test_predict = gru_model.predict_prob(test_point_list)\n    gru_model.plot_confusion_matrix(test_labels,gru_test_predict, labels_name, cm_title)\n    print(\"GRU Video model accuracy: \", accuracy_score(test_labels, gru_test_predict), \"\\n\\n\")\n\n    # Load parameters of LSTM video model, make the predictions and plot the confusion matrix\n    lstm_model = LSTMEmotionRecognitionModel(input_shape=(50, 709), num_classes=NUM_CLASSES)\n    lstm_model.model.load_weights(lstm_path)\n    lstm_test_predict_probs, lstm_test_predict = lstm_model.predict_prob(test_point_list)\n    lstm_model.plot_confusion_matrix(test_labels,lstm_test_predict, labels_name, cm_title)\n    print(\"LSTM Video model accuracy: \",accuracy_score(test_labels, lstm_test_predict), \"\\n\\n\")\n    \n    return gru_test_predict_probs, gru_test_predict, lstm_test_predict_probs, lstm_test_predict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.455634Z","iopub.execute_input":"2025-03-05T18:40:35.455956Z","iopub.status.idle":"2025-03-05T18:40:35.475033Z","shell.execute_reply.started":"2025-03-05T18:40:35.455926Z","shell.execute_reply":"2025-03-05T18:40:35.474249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio extraction by Video","metadata":{}},{"cell_type":"code","source":"def extract_audio_from_videos(files, output_directory):\n    \"\"\"Extracts audios from videos and it saves them into the output directory\n\n    Args:\n        files: list of video's paths  \n        output_directory: directory where to save audios\n        \n    Return:\n        out_paths (list): audio paths extracted and saved\n    \"\"\"\n    out_paths = []\n    rav = output_directory + \"/\" + \"ravdess\"\n    \n    os.makedirs(output_directory, exist_ok=True)\n    os.makedirs(rav, exist_ok=True)\n    \n    i = 0\n    for video_path in tqdm(files, desc = \"Extracting audios\"):\n        filename = video_path.split(\"/\")[-1]\n        try:\n            \n            audio_path = os.path.join(rav, filename.split(\".\")[0]+\"_\"+ str(i) +\".wav\")\n            out_paths.append(audio_path)\n            \n            # if already exists the audio in the directory don't re-extract it\n            if not os.path.exists(audio_path):\n                audio = AudioSegment.from_file(video_path, format=\"mp4\")\n                audio.export(audio_path, format=\"wav\")\n            i += 1\n        except Exception as e:\n            print(f\"Failed to process {video_path}: {e}\")\n    return out_paths\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.475907Z","iopub.execute_input":"2025-03-05T18:40:35.476258Z","iopub.status.idle":"2025-03-05T18:40:35.490911Z","shell.execute_reply.started":"2025-03-05T18:40:35.476235Z","shell.execute_reply":"2025-03-05T18:40:35.490233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_audio(file_path):\n    \"\"\"Loads the audio from the .wav file\n\n    Args:\n        file_path: audio file path\n        \n    Return:\n        tuple: A tuple containing:\n            - y: The audio time series as a numpy array.\n            - sr: The sampling rate of the audio file.\n\n    \"\"\"\n    # Set the sampling rate to 22050 Hz\n    sr=22050\n\n    # Extract the dataset name from the file path\n    dataset = file_path.split(\"/\")[-2]\n\n     # Set offset and duration for the \"ravdess\" dataset\n    if dataset == \"ravdess\":\n        offset = 1.8\n        duration = 2\n    try:\n        # Load the audio file with the specified parameters\n        y, sr = librosa.load(file_path, sr=sr, duration=duration, offset=offset)\n        return y, sr\n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file_path)\n        return None, None\n\ndef extract_MFCC(file_path, n_mfcc=64):\n    \"\"\"Extracts Mel-Frequency Cepstral Coefficients (MFCC) from an audio file.\n\n    Args:\n        file_path: Path to the audio file from which to extract MFCCs.\n        n_mfcc: Number of MFCCs to return. Defaults to 64.\n\n    Returns:\n        A 2D numpy array containing the MFCC spectrogram, or None if the audio file could not be loaded.\n    \"\"\"\n    # Load the audio file\n    y, sr = load_audio(file_path)\n\n    # Check if the audio file was loaded successfully\n    if y is None:\n        return None\n\n    # Extract MFCC features from the audio signal\n    mfcc_spectrogram = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n\n    # Ensure the MFCC spectrogram has the desired shape\n    if mfcc_spectrogram is not None:\n        if mfcc_spectrogram.shape != (64, 87):\n            # Trim or pad the spectrogram to match the desired shape\n            if mfcc_spectrogram.shape[1] > 87:\n                mfcc_spectrogram = mfcc_spectrogram[:, :87]\n            else:\n                mfcc_spectrogram = np.pad(mfcc_spectrogram,\n                                          ((0, 0), (0, 87 - mfcc_spectrogram.shape[1])),\n                                          mode='constant')\n    return mfcc_spectrogram\n\ndef dump_spectrogram(mel_spectrogram, output_file):\n    \"\"\"Saves a Mel spectrogram to a file.\n\n    Args:\n        mel_spectrogram: The Mel spectrogram to save.\n        output_file: The path to the output file where the spectrogram will be saved.\n    \"\"\"\n    # Create the output directory if it does not exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n    # Save the Mel spectrogram to the specified file\n    np.save(output_file, mel_spectrogram)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.491825Z","iopub.execute_input":"2025-03-05T18:40:35.492053Z","iopub.status.idle":"2025-03-05T18:40:35.508602Z","shell.execute_reply.started":"2025-03-05T18:40:35.492017Z","shell.execute_reply":"2025-03-05T18:40:35.507843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_dataset(files, output_folder):\n    \"\"\"Preprocesses a dataset of audio files and saves the extracted features.\n\n    Args:\n        files: List of file paths to the audio files to be processed.\n        output_folder: The folder where the extracted features will be saved.\n\n    Returns:\n        features_path: A list of paths to the saved feature files.\n    \"\"\"\n    cont = 0  # Counter for the number of processed files\n    ravdess_dict = {0: 'ne', 1: 'ca', 2: 'ha', 3: 'sa', 4: 'an', 5: 'fe', 6: 'di', 7: 'su'}\n    features_path = []  # List to store the paths of the saved feature files\n\n    for f in files:\n        directory = f.split(\"/\")[-2]  # Extract the directory name from the file path\n        filename = f.split(\"/\")[-1].split(\".\")[0]  # Extract the filename without extension\n\n        if directory == \"ravdess\":  # Process only if the directory is \"ravdess\"\n            part = filename.split('-')  \n            emotion = ravdess_dict[int(part[2]) - 1]  # Determine the emotion from the filename\n            audio_file = f  \n\n            # Define the output file path for the extracted features\n            output_file = os.path.join(\n                output_folder, emotion,\n                filename + \".npy\",\n            )\n\n            # Check if the output file already exists\n            if not os.path.exists(output_file):\n                # Extract MFCC features from the audio file\n                mfcc_spectrogram = extract_MFCC(audio_file)\n                # Save the extracted features to the output file\n                dump_spectrogram(mfcc_spectrogram, output_file)\n\n            cont += 1  \n\n        features_path.append(output_file)  \n\n    return features_path  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.509377Z","iopub.execute_input":"2025-03-05T18:40:35.509665Z","iopub.status.idle":"2025-03-05T18:40:35.527254Z","shell.execute_reply.started":"2025-03-05T18:40:35.509643Z","shell.execute_reply":"2025-03-05T18:40:35.526511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import soundfile as sf\nfrom librosa.effects import pitch_shift, time_stretch\n\ndef augmentation(audio_paths):\n    \"\"\"Performs data augmentation on a list of audio files.\n\n    Args:\n        audio_paths: List of paths to the audio files to be augmented.\n\n    Returns:\n        new_audios_paths: A list of paths to the augmented audio files, including the originals.\n    \"\"\"\n    new_audios_paths = []  # List to store paths of the augmented audio files\n\n    for audio in tqdm(audio_paths, desc=\"Audio augmentation..\"):\n        # Load the audio file with a fixed sampling rate of 22050 Hz\n        y, sr = librosa.load(audio, sr=22050)\n        base_name = os.path.splitext(audio)[0]  # Base name of the audio file without extension\n\n        # 1. Pitch Shifting: Shifts the pitch by Â±4 semitones (excluding 0)\n        for steps in range(-4, 5):\n            if steps == 0:\n                continue\n            output_path = f\"{base_name}_pitch_{steps}.wav\"\n            if not os.path.exists(output_path):\n                y_shifted = pitch_shift(y, sr=sr, n_steps=steps)\n                sf.write(output_path, y_shifted, sr)\n            new_audios_paths.append(output_path)\n\n        # 2. Time Stretching: Modifies the playback speed\n        for rate in [0.9, 1.1]:  # 0.9 slows down, 1.1 speeds up\n            output_path = f\"{base_name}_timestretch_{rate}.wav\"\n            if not os.path.exists(output_path):\n                y_stretched = time_stretch(y, rate=rate)\n                sf.write(output_path, y_stretched, sr)\n            new_audios_paths.append(output_path)\n\n        # 3. Noise Injection: Adds Gaussian noise to the audio\n        noise_factor = 0.005  \n        output_path = f\"{base_name}_noise.wav\"\n        if not os.path.exists(output_path):\n            noise = np.random.randn(len(y))\n            y_noisy = y + noise_factor * noise\n            sf.write(output_path, y_noisy, sr)\n        new_audios_paths.append(output_path)\n\n        # 4. Volume Adjustment: Modifies the volume by scaling\n        for factor in [0.8, 1.2]:  # 0.8 reduces volume, 1.2 increases volume\n            output_path = f\"{base_name}_vol_{factor}.wav\"\n            if not os.path.exists(output_path):\n                y_vol = y * factor\n                sf.write(output_path, y_vol, sr)\n            new_audios_paths.append(output_path)\n\n    # Add the original audio files to the output list\n    new_audios_paths += audio_paths\n    return new_audios_paths\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.528040Z","iopub.execute_input":"2025-03-05T18:40:35.528324Z","iopub.status.idle":"2025-03-05T18:40:35.542949Z","shell.execute_reply.started":"2025-03-05T18:40:35.528305Z","shell.execute_reply":"2025-03-05T18:40:35.542248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass MultilabelSpectrogramDataset(Dataset):\n    \"\"\"A custom dataset class for handling spectrogram data with multi-labels.\n\n    Args:\n        file_paths: List of file paths to the spectrogram data.\n        labels: List of labels corresponding to each spectrogram.\n    \"\"\"\n\n    def __init__(self, file_paths, labels):\n        \"\"\"Initializes the dataset with file paths and corresponding labels.\"\"\"\n        self.file_paths = file_paths  # List of paths to the spectrogram files\n        self.labels = labels  # List of labels for each spectrogram\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves a sample from the dataset at the given index.\n\n        Args:\n            idx: Index of the sample to retrieve.\n\n        Returns:\n            A tuple containing the spectrogram and its corresponding label as tensors.\n        \"\"\"\n        file_path = self.file_paths[idx]  # Get the file path for the current index\n        spectrogram = np.load(file_path)  # Load the spectrogram from the file\n        label = self.labels[idx]  # Get the corresponding label\n\n        # Convert the spectrogram and label to PyTorch tensors\n        return torch.tensor(spectrogram, dtype=torch.float32), torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.543819Z","iopub.execute_input":"2025-03-05T18:40:35.544168Z","iopub.status.idle":"2025-03-05T18:40:35.580453Z","shell.execute_reply.started":"2025-03-05T18:40:35.544136Z","shell.execute_reply":"2025-03-05T18:40:35.579667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_audio_dataset(files):\n    \"\"\"Loads labels for an audio dataset based on file paths.\n\n    Args:\n        files: List of file paths to the audio files.\n\n    Returns:\n        labels: A list of labels corresponding to each audio file.\n    \"\"\"\n    labels = []\n    for file in files:\n        label_name = file.split(\"/\")[-2]\n        label = emotions_to_idx[label_name]\n        labels.append(label)\n    return labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.581306Z","iopub.execute_input":"2025-03-05T18:40:35.581632Z","iopub.status.idle":"2025-03-05T18:40:35.594732Z","shell.execute_reply.started":"2025-03-05T18:40:35.581602Z","shell.execute_reply":"2025-03-05T18:40:35.593899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_audio_dataloader(data_audio):\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.595655Z","iopub.execute_input":"2025-03-05T18:40:35.595906Z","iopub.status.idle":"2025-03-05T18:40:35.612614Z","shell.execute_reply.started":"2025-03-05T18:40:35.595886Z","shell.execute_reply":"2025-03-05T18:40:35.611287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass AudioModel(nn.Module):\n    \"\"\"A convolutional neural network model for audio classification.\n\n    Args:\n        num_labels: Number of output labels (default is 8).\n    \"\"\"\n\n    def __init__(self, num_labels=8):\n        \"\"\"Initializes the AudioModel with specified number of output labels.\"\"\"\n        super(AudioModel, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=\"same\")\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=\"same\")\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=\"same\")\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=\"same\")\n\n        # Max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Activation function\n        self.relu = nn.ReLU()\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 5, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(0.1)\n\n        # Output layer\n        self.fc4 = nn.Linear(128, num_labels)\n\n        # Softmax activation for multi-class classification\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \n        # Add a channel dimension to the input tensor\n        x = x.unsqueeze(1)\n\n        # Convolutional layers with ReLU activation and max pooling\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv3(x))\n        x = self.pool(x)\n\n        x = self.relu(self.conv4(x))\n        x = self.pool(x)\n\n        # Flatten the tensor for the fully connected layers\n        x = x.flatten(start_dim=1)\n\n        # Fully connected layers with ReLU activation and dropout\n        x1 = self.fc1(x)\n        x1 = self.relu(x1)\n        x1 = self.dropout(x1)\n\n        x2 = self.fc2(x1)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n\n        x3 = self.fc3(x2)\n        x3 = self.relu(x3)\n        x3 = self.dropout(x3)\n\n        # Final prediction layer with softmax activation\n        x4 = self.fc4(x3)\n        x5 = self.softmax(x4)\n\n        # Return the penultimate layer output and the final softmax output\n        return x3, x5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.613473Z","iopub.status.idle":"2025-03-05T18:40:35.613885Z","shell.execute_reply":"2025-03-05T18:40:35.613701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train loop audio model","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ndef train_audio_model(subtrain_dataloader, subtest_dataloader):\n    \"\"\"Trains the audio classification model and evaluates its performance.\n\n    Args:\n        subtrain_dataloader: DataLoader for the training dataset.\n        subtest_dataloader: DataLoader for the validation dataset.\n    \"\"\"\n    # Check if the model file already exists\n    if not os.path.exists('/kaggle/working/models/audio_model.pth'):\n\n        # Initialize the model\n        model = AudioModel()\n\n        # Label names for plotting\n        labels_list = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n        num_labels = len(labels_list)\n\n        # Initialize a confusion matrix\n        confusion_matrix_ = np.zeros((num_labels, num_labels))\n\n        # Create the model directory if it does not exist\n        os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n\n        # Set the device for training (GPU if available, otherwise CPU)\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n\n        # Define the loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n\n        # Lists to store training losses and accuracy scores\n        losses = []\n        accuracy_scores = dict()\n\n        # Number of training epochs\n        num_epochs = 150\n\n        # Training loop\n        for epoch in range(num_epochs):\n            model.train()  # Set the model to training mode\n\n            total_loss = 0.0\n\n            # Iterate over the training dataset\n            for audios, labels in tqdm(subtrain_dataloader):\n                audios = audios.to(device)\n                labels = labels.to(device)\n\n                # Forward pass\n                features, outputs = model(audios)\n                loss = criterion(outputs, labels)\n\n                # Backward pass and optimization\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}')\n\n            model.eval()  # Set the model to evaluation mode\n\n            all_preds = []\n            all_labels = []\n\n            # Evaluate the model on the validation dataset\n            for audios, labels in subtest_dataloader:\n                audios = audios.to(device)\n                labels = labels.to(device)\n\n                # Forward pass\n                features, outputs = model(audios)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n                # Update the confusion matrix\n                confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n            # Calculate validation accuracy\n            accuracy = accuracy_score(all_labels, all_preds)\n            print(f'Validation accuracy: {accuracy}')\n\n            # Store the model state with the highest accuracy\n            accuracy_scores[accuracy] = model.state_dict()\n\n            losses.append(total_loss)\n\n        # Find the best model based on validation accuracy\n        best_accuracy = max(accuracy_scores.keys())\n        best_model = accuracy_scores[best_accuracy]\n        print(\"\\nBest validation accuracy: \", best_accuracy, \"\\n\")\n\n        # Save the best model\n        torch.save(best_model, '/kaggle/working/models/audio_model.pth')\n        accuracy_scores = list(accuracy_scores.keys())\n\n        # Plot training loss\n        plt.plot(losses)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Audio Training Loss')\n        plt.show()\n\n        # Plot validation accuracy\n        plt.plot(accuracy_scores)\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Audio Validation Accuracy')\n        plt.show()\n\n        # Normalize the confusion matrix\n        confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n        # Plot the confusion matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap='Purples')\n        plt.title('Audio Prediction on Subtest Set in Training Mode')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:40:35.614781Z","iopub.status.idle":"2025-03-05T18:40:35.615159Z","shell.execute_reply":"2025-03-05T18:40:35.614967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef create_audio_model(subtrain_data, subtest_data, test_data):\n    \"\"\"Creates and trains an audio classification model using video data.\n\n    Args:\n        subtrain_data: List of video paths for the training set.\n        subtest_data: List of video paths for the validation set.\n        test_data: List of video paths for the test set.\n    Returns:\n        Dataloaders of training, validation and test set\n    \"\"\"\n    # Extract audio from videos in the subtrain set\n    print(\"Audio extraction by video of subtrain set...\\n\")\n    audio_paths_subtrain = extract_audio_from_videos(subtrain_data, '/kaggle/working/Audios/subtrain')\n    \n    # Perform data augmentation on the extracted audio files\n    #audio_paths_subtrain = augmentation(audio_paths_subtrain)\n\n    # Extract audio from videos in the subtest set\n    print(\"Audio extraction by video of subtest set...\\n\")\n    audio_paths_subtest = extract_audio_from_videos(subtest_data, '/kaggle/working/Audios/subtest')\n\n    print(\"Audio extraction by video of test set...\\n\")\n    audio_paths_test = extract_audio_from_videos(test_data, '/kaggle/working/Audios/test')\n\n    # Extract features from the audio files in the subtrain set\n    print(\"Features extraction by audio of subtrain set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/subtrain\"\n    features_paths_subtrain = preprocess_dataset(audio_paths_subtrain, output_folder)\n\n    # Extract features from the audio files in the subtest set\n    print(\"Features extraction by audio of subtest set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/subtest\"\n    features_paths_subtest = preprocess_dataset(audio_paths_subtest, output_folder)\n\n    # Extract features from the audio files in the test set\n    print(\"Features extraction by audio of test set...\\n\")\n    output_folder = \"/kaggle/working/Dataset/test\"\n    features_paths_test = preprocess_dataset(audio_paths_test, output_folder)\n\n    # Extract labels for the features\n    subtrain_labels = load_audio_dataset(features_paths_subtrain)\n    subtest_labels = load_audio_dataset(features_paths_subtest)\n    test_labels = load_audio_dataset(features_paths_test)\n    \n    # Define the batch size for training and evaluation\n    BATCH_SIZE = 64\n\n    # Create datasets for the subtrain and subtest sets\n    subtrain_dataset = MultilabelSpectrogramDataset(features_paths_subtrain, subtrain_labels)\n    subtest_dataset = MultilabelSpectrogramDataset(features_paths_subtest, subtest_labels)\n    test_dataset = MultilabelSpectrogramDataset(features_paths_test, test_labels)\n    \n    # Create DataLoaders for the subtrain and subtest datasets\n    subtrain_dataloader = DataLoader(subtrain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    subtest_dataloader = DataLoader(subtest_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Train the audio model\n    print(\"Training audio model...\\n\")\n    train_audio_model(subtrain_dataloader, subtest_dataloader)\n\n    # Return the DataLoaders\n    return subtrain_dataloader, subtest_dataloader, test_dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:28:51.582662Z","iopub.execute_input":"2025-03-05T19:28:51.582984Z","iopub.status.idle":"2025-03-05T19:28:51.590016Z","shell.execute_reply.started":"2025-03-05T19:28:51.582960Z","shell.execute_reply":"2025-03-05T19:28:51.589129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def audio_predict(dataloader_audio, name=\"Test\"):\n    \"\"\"Predicts labels for audio data using a pre-trained model and evaluates its performance.\n\n    Args:\n        dataloader_audio: DataLoader audio dataset to test.\n        name: name of the set\n\n    Returns:\n        A tuple containing predictions and probabilities for the set tested\n    \"\"\"\n    # Load the pre-trained audio model\n    print(\"Loading of audio model by file...\\n\")\n    model_path = '/kaggle/working/models/audio_model.pth'\n    model = AudioModel()\n    model.load_state_dict(torch.load(model_path))\n\n    # Set the device for evaluation (GPU if available, otherwise CPU)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    # Label names for plotting\n    labels_list = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n    num_labels = len(labels_list)\n\n    # Initialize variables for storing predictions and features\n    confusion_matrix_ = np.zeros((num_labels, num_labels))\n    all_labels = []\n    all_outputs = []\n    all_preds = []\n\n    # Evaluate the model on the subtest dataset\n    for audios, labels in dataloader_audio:\n        audios = audios.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        features, outputs = model(audios)\n        all_outputs.extend(outputs.cpu().detach().numpy())\n        _, preds = torch.max(outputs, 1)\n\n        # Store predictions, labels, and features\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n        # Update the confusion matrix\n        confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n    # Calculate accuracy for the subtest set\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f'\\n\\nAccuracy of audio model on {name} set: {accuracy}\\n')\n\n    # Normalize the confusion matrix\n    confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n    # Plot the confusion matrix for the subtest set\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap=\"Purples\")\n    plt.title('Audio Prediction on', name,'Set')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n    # Returns predictions and probabilities\n    return all_preds, all_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:31:02.551526Z","iopub.execute_input":"2025-03-05T19:31:02.551867Z","iopub.status.idle":"2025-03-05T19:31:02.560523Z","shell.execute_reply.started":"2025-03-05T19:31:02.551841Z","shell.execute_reply":"2025-03-05T19:31:02.559484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Audio Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass VideoAudioDataset(Dataset):\n    \"\"\"A custom dataset class for handling predictions and corresponding labels.\n\n    Args:\n        preds: List of predictions.\n        labels: List of labels corresponding to each prediction.\n    \"\"\"\n\n    def __init__(self, preds, labels):\n        \"\"\"Initializes the dataset with predictions and corresponding labels.\"\"\"\n        self.preds = preds  # List of predictions\n        self.labels = labels  # List of labels\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves a sample from the dataset at the given index.\n\n        Args:\n            idx: Index of the sample to retrieve.\n\n        Returns:\n            A tuple containing the prediction and its corresponding label as tensors.\n        \"\"\"\n        pred = self.preds[idx]  # Get the prediction for the current index\n        label = self.labels[idx]  # Get the corresponding label\n\n        # Convert the prediction and label to PyTorch tensors\n        return torch.tensor(pred, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:31:41.846608Z","iopub.execute_input":"2025-03-05T19:31:41.846936Z","iopub.status.idle":"2025-03-05T19:31:41.852573Z","shell.execute_reply.started":"2025-03-05T19:31:41.846912Z","shell.execute_reply":"2025-03-05T19:31:41.851789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Audio Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nclass VideoAudioModel(nn.Module):\n    \"\"\"A neural network model for audio-video classification.\n\n    Args:\n        num_labels: Number of output labels (default is 8).\n    \"\"\"\n\n    def __init__(self, num_labels=8):\n        \"\"\"Initializes the VideoAudioModel with specified number of output labels.\"\"\"\n        super(VideoAudioModel, self).__init__()\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(24, 128)\n        self.fc6 = nn.Linear(128, num_labels)\n\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        \"\"\"Defines the forward pass of the VideoAudioModel.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output tensor after passing through the network.\n        \"\"\"\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.fc6(x)\n        return x\n\n    def train_model(self, train_dataloader, test_dataloader):\n        \"\"\"Trains the model and evaluates its performance.\n\n        Args:\n            train_dataloader: DataLoader for the training dataset.\n            test_dataloader: DataLoader for the validation dataset.\n        \"\"\"\n        # Label names for plotting\n        labels_list = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n        num_labels = len(labels_list)\n\n        # Initialize a confusion matrix\n        confusion_matrix_ = np.zeros((num_labels, num_labels))\n\n        # Set the device for training (GPU if available, otherwise CPU)\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.to(device)\n\n        # Lists to store training losses and accuracy scores\n        losses = []\n        accuracy_scores = dict()\n\n        # Number of training epochs\n        num_epochs = 100\n\n        # Define the loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.001)\n\n        # Store the initial model state\n        last_model = self.state_dict()\n\n        # Training loop\n        for epoch in range(num_epochs):\n            self.train()  # Set the model to training mode\n\n            total_loss = 0.0\n\n            # Iterate over the training dataset\n            for inputs, labels in tqdm(train_dataloader):\n                pred = inputs.to(device)\n                labels = labels.to(device)\n\n                # Forward pass\n                outputs = self(pred)\n                loss = criterion(outputs, labels)\n\n                # Backward pass and optimization\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}')\n\n            self.eval()  # Set the model to evaluation mode\n\n            all_preds = []\n            all_labels = []\n\n            # Evaluate the model on the validation dataset\n            for inputs, labels in test_dataloader:\n                pred = inputs.to(device)\n                labels = labels.to(device)\n\n                # Forward pass\n                outputs = self(pred)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n                # Update the confusion matrix\n                confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n            # Calculate validation accuracy\n            accuracy = accuracy_score(all_labels, all_preds)\n            print(f'Validation accuracy: {accuracy}')\n\n            # Store the model state with the highest accuracy\n            accuracy_scores[accuracy] = self.state_dict()\n\n            losses.append(total_loss)\n\n            # Track the best model based on validation accuracy\n            best_accuracy = max(accuracy_scores.keys())\n            last_model = self.state_dict()\n\n        # Save the best model\n        best_model = accuracy_scores[best_accuracy]\n        print(\"\\n\\nBest validation Audio-Video model accuracy: \", best_accuracy, \"\\n\")\n        torch.save(best_model, '/kaggle/working/models/audio_video_model.pth')\n\n        # Save the last model state\n        torch.save(last_model, '/kaggle/working/models/last_epoch_audio_video_model.pth')\n\n        # Plot training loss\n        plt.plot(losses)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Audio-Video Training Loss')\n        plt.show()\n\n        # Plot validation accuracy\n        plt.plot(list(accuracy_scores.keys()))\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Audio-Video Validation Accuracy')\n        plt.show()\n\n        # Normalize the confusion matrix\n        confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n        # Plot the confusion matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap=\"Blues\")\n        plt.title('Video-Audio Prediction on Test Set in Training Mode')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n\n    def test_model(self, test_dataloader):\n        \"\"\"Evaluates the model on the test dataset.\n\n        Args:\n            test_dataloader: DataLoader for the test dataset.\n\n        Returns:\n            A tuple containing all predictions and outputs.\n        \"\"\"\n        # Label names for plotting\n        labels_list = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgusted', 'Surprised']\n        num_labels = len(labels_list)\n\n        # Initialize a confusion matrix\n        confusion_matrix_ = np.zeros((num_labels, num_labels))\n\n        # Set the device for evaluation (GPU if available, otherwise CPU)\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.to(device)\n\n        # Load the best model state\n        model_path = '/kaggle/working/models/audio_video_model.pth'\n        self.load_state_dict(torch.load(model_path))\n        self.eval()\n\n        all_preds = []\n        all_labels = []\n        all_outputs = []\n\n        # Evaluate the model on the test dataset\n        for inputs, labels in test_dataloader:\n            pred = inputs.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = self(pred)\n            all_outputs.append(outputs)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            # Update the confusion matrix\n            confusion_matrix_ += confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy(), labels=range(num_labels))\n\n        # Calculate test accuracy\n        accuracy = accuracy_score(all_labels, all_preds)\n        print(f'\\n\\nAudio-Video model accuracy on test set: {accuracy}')\n\n        # Normalize the confusion matrix\n        confusion_matrix_ = confusion_matrix_ / np.sum(confusion_matrix_, axis=1, keepdims=True)\n\n        # Plot the confusion matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(confusion_matrix_, annot=True, fmt=\".2%\", xticklabels=labels_list, yticklabels=labels_list, cmap=\"Blues\")\n        plt.title('Video-Audio Prediction on Test Set')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n\n        # Return all predictions and outputs\n        return all_preds, all_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:33:03.406587Z","iopub.execute_input":"2025-03-05T19:33:03.406929Z","iopub.status.idle":"2025-03-05T19:33:03.836434Z","shell.execute_reply.started":"2025-03-05T19:33:03.406907Z","shell.execute_reply":"2025-03-05T19:33:03.835639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Function","metadata":{}},{"cell_type":"code","source":"# Load video/audio paths from dataset\npath_audio, labels = load_dataset('/kaggle/input/ravdess-emotional-speech-video/RAVDESS dataset')\n\n# Divide dataset into subtra<in dataset to train audio and video models, \n# subtest dataset to test audio and video models and train the combined model, test the combined model\ntrain_data_audio, test_data_audio, train_labels, test_labels = train_test_split(path_audio, labels, test_size=0.2, random_state= 15)\nsubtrain_data_audio, subtest_data_audio, subtrain_labels, subtest_labels = train_test_split(train_data_audio, train_labels, test_size=0.3, random_state= 15)\n\n# Obtain from video/audio paths to features video paths\ntrain_data_video = from_audiopath_to_videopath(train_data_audio)\ntest_data_video = from_audiopath_to_videopath(test_data_audio)\nsubtrain_data_video = from_audiopath_to_videopath(subtrain_data_audio)\nsubtest_data_video = from_audiopath_to_videopath(subtest_data_audio)\n\nprint(f\"Audio-video model test size: audio {len(test_data_audio)}, video {len(test_data_video)}\")\nprint(f\"Submodels train size: audio {len(subtrain_data_audio)}, video {len(subtrain_data_video)}\")\nprint(f\"Submodels test size: audio {len(subtest_data_audio)}, video {len(subtest_data_video)}\")\n\nprint()\n\nemotions_to_idx = {'ne':0, 'ca': 1, 'ha': 2, 'sa': 3, 'an': 4, 'fe': 5, 'di': 6, 'su': 7}\n\n# Train GRU and LSTM video model\nscaler = train_video_model(subtrain_data_video, subtest_data_video, emotions_to_idx)\n\n# Test GRU and LSTM video model and obtain prediction logits for subtest and test dataset used by the combined model\ngru_subtest_video_predict_probs, gru_subtest_video_predict, lstm_subtest_video_predict_probs, lstm_subtest_video_predict = video_predict(scaler, subtest_data_video, '/kaggle/working/Video_track/subtest_features_rav.pkl', emotions_to_idx, \"Video prediction on subtest set\")\ngru_test_video_predict_probs, gru_test_video_predict, lstm_test_video_predict_probs, lstm_test_video_predict = video_predict(scaler, test_data_video, '/kaggle/working/Video_track/test_features_rav.pkl', emotions_to_idx, \"Video prediction on subtest set\")\n\n\n# Train audio model\nsubtrain_dataloader_audio, subtest_dataloader_audio, test_dataloader_audio = create_audio_model(subtrain_data_audio, subtest_data_audio, test_data_audio)\n# Test audio model and obtain prediction logits for subtest and test dataset used by the combined model\nsubtest_audio_predict, subtest_audio_predict_probs = audio_predict(subtest_dataloader_audio, name=\"Subtest\")\ntest_audio_predict, test_audio_predict_probs = audio_predict(test_dataloader_audio, name=\"Test\")\n\n# Stack the video and audio predictions for subtest dataset to train the combined model\nlstm_subtest_video_predict_numpy = np.array(lstm_subtest_video_predict_probs)\ngru_subtest_video_predict_numpy = np.array(gru_subtest_video_predict_probs)\nsubtest_audio_predict_numpy = np.array(subtest_audio_predict_probs)\nsubtest_audioVideo_predict = np.column_stack((gru_subtest_video_predict_numpy, lstm_subtest_video_predict_numpy, subtest_audio_predict_numpy))\n\n# Stack the video and audio predictions for test dataset to train the combined model\ngru_test_video_predict_numpy = np.array(gru_test_video_predict_probs)\nlstm_test_video_predict_numpy = np.array(lstm_test_video_predict_probs)\ntest_audio_predict_numpy = np.array(test_audio_predict_probs)\ntest_audioVideo_predict = np.column_stack((gru_test_video_predict_numpy, lstm_test_video_predict_numpy, test_audio_predict_numpy))\n\n# Create VideoAudio dataset and dataloader for the combined model\nsubtest_dataset = VideoAudioDataset(subtest_audioVideo_predict, subtest_labels)\ntest_dataset = VideoAudioDataset(test_audioVideo_predict, test_labels)\n\nBATCH_SIZE = 16\n\nsubtest_dataloader = DataLoader(subtest_dataset, batch_size=BATCH_SIZE, shuffle = False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = False)\n\n# Train and Test audio model\naudioVideo_model = VideoAudioModel()\naudioVideo_model.train_model(subtest_dataloader, test_dataloader)\ndef_preds, def_outputs = audioVideo_model.test_model(test_dataloader)\n\n# Show grafic for correctness prediction of the models\ntrue_labels = np.array(test_labels)\npredictions_model_1 = np.array(test_audio_predict)\npredictions_model_2 = np.array(lstm_test_video_predict)\npredictions_model_3 = np.array(gru_test_video_predict)\npredictions_model_4 = np.array(def_preds)\n\ncorrectness = np.zeros((len(test_labels), 4)) \n\ncorrectness[:, 0] = (predictions_model_1 == true_labels)\ncorrectness[:, 1] = (predictions_model_2 == true_labels)\ncorrectness[:, 2] = (predictions_model_3 == true_labels)\ncorrectness[:, 3] = (predictions_model_4 == true_labels)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(correctness, annot=False, cmap='Greens', cbar=False, xticklabels=['Audio', 'Video lstm', 'Video gru', 'Audio-Video'], yticklabels=[])\nplt.xlabel('Model')\nplt.ylabel('Predictions')\nplt.title('Correctness of Predictions by Models')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:34:32.267667Z","iopub.execute_input":"2025-03-05T19:34:32.268112Z","iopub.status.idle":"2025-03-05T19:34:32.391473Z","shell.execute_reply.started":"2025-03-05T19:34:32.268069Z","shell.execute_reply":"2025-03-05T19:34:32.390154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predictions Analysis","metadata":{}},{"cell_type":"code","source":"true_labels = np.array(test_labels)  # True labels\nensemble_preds = np.array(def_preds)  # Final predictions of the Ensemble\n\n# Predictions of individual models\npredictions = {\n    \"Audio\": np.array(test_audio_predict),\n    \"LSTM\": np.array(lstm_test_video_predict),\n    \"GRU\": np.array(gru_test_video_predict),\n}\n\ntotal_samples = len(true_labels)\n\n# Initialize counters\npositive_contributions = {model: 0 for model in predictions}\nnegative_influence = {model: 0 for model in predictions}\npositive_exclusive = {model: 0 for model in predictions}\nnegative_exclusive = {model: 0 for model in predictions}\nmodel_correct_ensemble_wrong = {model: 0 for model in predictions}\n\n# Analysis of predictions\nfor model, preds in predictions.items():\n    positive_contributions[model] = np.sum((preds == ensemble_preds) & (ensemble_preds == true_labels))\n    negative_influence[model] = np.sum((preds == ensemble_preds) & (ensemble_preds != true_labels))\n\n    # Exclusive influence\n    for i in range(total_samples):\n        other_models = [predictions[m][i] for m in predictions if m != model]\n\n        if preds[i] == ensemble_preds[i]:\n            if ensemble_preds[i] == true_labels[i]:\n                if all(other != preds[i] for other in other_models):\n                    positive_exclusive[model] += 1\n            else:\n                if all(other != preds[i] for other in other_models):\n                    negative_exclusive[model] += 1\n\n        # New metric: the model was correct, but the ensemble was wrong\n        if preds[i] == true_labels[i] and ensemble_preds[i] != true_labels[i]:\n            model_correct_ensemble_wrong[model] += 1\n\nprint(f\"The ensemble model predicted the correct label {np.sum(ensemble_preds == true_labels)/total_samples}% of the samples\")\nprint(\"-\" * 50)\n# Convert to percentage\nfor model in predictions:\n    positive_contributions[model] = (positive_contributions[model] / total_samples) * 100\n    negative_influence[model] = (negative_influence[model] / total_samples) * 100\n    positive_exclusive[model] = (positive_exclusive[model] / total_samples) * 100\n    negative_exclusive[model] = (negative_exclusive[model] / total_samples) * 100\n    model_correct_ensemble_wrong[model] = (model_correct_ensemble_wrong[model] / total_samples) * 100\n\n    print(f\"{model} contributed positively {positive_contributions[model]:.2f}% of the time\")\n    print(f\"{model} influenced negatively {negative_influence[model]:.2f}% of the time\")\n    print(f\"{model} influenced positively exclusively {positive_exclusive[model]:.2f}% of the time\")\n    print(f\"{model} influenced negatively exclusively {negative_exclusive[model]:.2f}% of the time\")\n    print(f\"{model} was correct while the Ensemble was wrong {model_correct_ensemble_wrong[model]:.2f}% of the time\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T19:44:25.972338Z","iopub.execute_input":"2025-03-05T19:44:25.972865Z","iopub.status.idle":"2025-03-05T19:44:26.010606Z","shell.execute_reply.started":"2025-03-05T19:44:25.972834Z","shell.execute_reply":"2025-03-05T19:44:26.009403Z"}},"outputs":[],"execution_count":null}]}